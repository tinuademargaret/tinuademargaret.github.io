<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://tinuademargaret.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://tinuademargaret.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-03-26T01:26:13+00:00</updated><id>https://tinuademargaret.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Book Review:World of Abstract Mathematics</title><link href="https://tinuademargaret.github.io/blog/2024/book-review/" rel="alternate" type="text/html" title="Book Review:World of Abstract Mathematics"/><published>2024-03-17T14:14:00+00:00</published><updated>2024-03-17T14:14:00+00:00</updated><id>https://tinuademargaret.github.io/blog/2024/book-review</id><content type="html" xml:base="https://tinuademargaret.github.io/blog/2024/book-review/"><![CDATA[<h2 id="my-background">My Background</h2> <p>My background in mathematics is very average, calculus in secondary school advanced calculus in my first and second year in uni, some statistics and that was it. But I was one of those kids that enjoyed the subject and wasn’t afraid of it, I also didn’t understand why people hated it. My best homework were maths problems. During my A levels, I solved all of the problems in my <a href="https://www.amazon.co.uk/Pure-Mathematics-Advanced-Level-Bunday/dp/0408700327/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">coursework textbook</a> for the fun of it! I would solve problems all the time, during my break time, while eating, while waiting for the next teacher to come to class, and when I didn’t feel like studying other subjects. I also led my maths tutorial group that was designed to help other students struggling with maths. Basically, maths has never been my problem but that was all it was to me learn the formula and steps to solve the problem plug in the numbers, recognise the curveballs etc. As I started to move further in my career first, in software engineering and now in AI research I realise that I never really understood the subject, like why a theorem existed and how to apply them in solving real world problems. I could get away with it as a SWE but now as an AI researcher I have realised that if I’m ever going to be able to make meaningful contributions to my field, then I have to fill up the gaps in my knowledge of the subject. This has led me on a journey to reading a series of mathematical textbooks the first of which is “World of Abstract Mathematics” which I’ll be reviewing in this post.</p> <h2 id="about-the-book">About the book</h2> <p>The book was written by Prof. Brendan Sullivan as a part of his thesis in the fulfilment of his PhD program at Carnegie Mellon University. It gives a good Introduction into fundamental theoretical concepts in mathematics such as set theory, logic, relations etc with a focus on mathematical proof techniques. It emphasises on the preciseness of the subject, and gives an insight into how mathematicians think about their work. In the next section, I’ll briefly go over each chapter, and in future posts I will delve deeper into some of the concepts I found interesting. If you’re aiming to thoroughly explore this book, this post serves as an excellent primer, since having a broad understanding of the subject matter you’re about to study is always beneficial.</p> <h2 id="chapter-summaries">Chapter Summaries</h2> <h3 id="chapter-1-what-is-mathematics">Chapter 1: What is mathematics?</h3> <p>I have always thought that there were no absolute truths, like how can we know something is true or always true when there are much more unknowns, magnitudes higher, than what we currently know. Also if we do something repeatedly 1000 times and it gives us the same result do we consider it to be always true? How do we know that the result would be the same the 1001th time we perform the experiment?</p> <p>As Prof. Brendan explains in this chapter, this is one of the questions mathematics seeks to answer, and it answers this question by mathematical proofs. So if you’ve ever wondered why mathematicians put themselves through what looks like a painful and really boring process of writing proofs here’s your answer. My mind also goes back to the movie “The Man Who Knew Infinity”, and it’s clearer now why Ramanujan’s supervisor was often in disagreement with him, despite his brilliance. Ramanujan relied heavily on intuition and often arrived at complex mathematical results without providing proofs that were standard practice in the mathematical community.</p> <p>Let’s look at Goldbach’s conjecture for example. It says that every even number greater than or equal to 4 can be written as the sum of two primes (yeah go ahead and try it for a couple of numbers) but there’s no mathematical proof for this and we can’t physically compute this for very large numbers hence there’s no way for us to tell that the mathematical statement is true.</p> <p>Devising new statements, finding if they are true or false (by making arguments) and sharing this with other people is what mathematics is about.</p> <p>Prof Brendan also did a good job of using various examples to show what a correct mathematical proof should constitute of; clarity and valid logical inferences at every step or claim in the proof.</p> <p>If you look closely at many almost “creative” and funny mathematical proofs, like those claiming 1 + 1 equals 1, or the mathematical proof of Xmas (as if mathematics acknowledges its divinity), you would find that they often rely on manipulation or incorrect statements. Many times these proofs assume a fact and deduce something is true from it but and there is a clear valid logical inference at every step but assuming a fact and then deducing something is true from it does not mean the assumed fact is true! In chapter 5 shows an example of how assumptions are made in a proof.</p> <p>The rest of this chapter briefly brushes through some mathematical concepts and techniques like Logic, Algebra, Polynomials, Sets, and some puzzles to show how mathematics can be applied to solve problems. One of the puzzles here is the full monty hall problem and Prof Brendan does a good job of explaining it so well (It’s hard to find resources that explains the solution to the problem very clearly imo).</p> <p>Many mathematicians, and myself are of the opinion that there are many truths out there waiting to be discovered, the only difference is that they spend their time learning about the truths that have already been discovered in the hopes of exposing more truths.</p> <p>P.S I found out in this chapter that Dr Brendan is also a fan of Top Gun Maverick which made me eager to listen to everything else he had to say in this book.</p> <h3 id="chapter-2-and-5-mathematical-induction">Chapter 2 and 5: Mathematical Induction</h3> <p>As we established in chapter 1 a lot of mathematics is finding truths via mathematical proofs, and a major focus of this book is mathematical proof techniques. There are proof techniques that have been repeatedly applied and generalised to argue certain kinds of mathematical problems or statements. One of the major proof techniques is mathematical induction which is basically making inductive arguments for problems that have an inductive structure i.e dependence of one part of the result to a previous result. This topic was first introduced in chapter 2 and then more rigorously discussed in chapter 5 after some necessary discussions on set theory and logic.</p> <p>Chapter 2 looked at inductive arguments where a fact depends on an immediate previous fact and arguments where a fact needs more information than a previous fact. </p> <p>Chapter 5 was a more formal introduction to the topic, mathematical induction as a principle was properly defined mathematically. The Principle of Mathematical Induction (PMI) says</p> <p>Let \(P(n)\) be some “fact” or “observation” that depends on the natural number \(n\). Assume that</p> <ol> <li>\(P(1)\) is a true statement</li> <li>Given any \(k ∈ N\), if \(P(k)\) is true, then we can conclude necessarily that \(P(k + 1)\) is true</li> </ol> <p>Then the statement \(P(n)\) must be true for every natural number \(n ∈ N\)</p> <p>Some questions that might pop up is what happens when the base case is not 1, what happens in a scenario where the inductive argument is backwards, these are the variants of induction. Another variant is inducting on even/odd numbers.</p> <p>Guess what, all we’ve discussed so far is known as regular induction and there is such a thing as strong induction. Let me pull a Prof. Brendan on you here, can you guess the difference?</p> <p>Well with regular induction we say that \(p(k)\) is sufficient for us to deduce \(p(k+1)\), but with strong induction \(p(1), p(2)...p(k)\) are all sufficient to deduce \(p(k+1)\). Here’s the Strong Principle Mathematical Induction</p> <p>Let \(P(n)\) be a variable proposition. Suppose that</p> <ol> <li>\(P(1)\) holds True, and</li> <li>\(∀k ∈ N .  ∀i ∈ [k] . P(i)  =&gt; P(k + 1)\) holds True</li> </ol> <p>Then \(∀n ∈ N\). \(P(n)\) holds True.</p> <p>Finally an equivalence relationship between PMI and SPMI is shown i.e whenever we need to prove something by PMI we can as well use SPMI, notice that the converse is also true. We’ll see what an equivalent relationship means in c4 but for now if A is equivalent to B, A implies B also means that B implies A.</p> <h3 id="chapter-3-set-theory">Chapter 3: Set Theory</h3> <p>You’ve probably heard that set theory is the foundation of mathematics, Prof. Brendan puts it this way “Everything we do in mathematics is built upon the foundation of sets” but based on what you did in high school and depending on which high school you went to you can probably only think of venn diagrams and you are wondering how venn diagrams can be the foundation that the mathematics is built upon. It might help you to drop the idea of Venn diagrams for a moment as you begin to read this chapter and get ready to take on a new perspective.</p> <p>Really what set theory does is that this it gives us a way to talk about mathematical objects and how to work with them. Let’s rewind back a little. Mathematics is about truth finding, to find truths we identify patterns, make hypothesis in form of mathematical statements, then we proof that this statements are true or false using logic and mathematical techniques based on existing truths(that have also been rigorously proved). In mathematical statements we talk about mathematical objects. Try to recall any formal mathematical theorem you know maybe Pythagoras theorem, you’d often here things like “the set of Natural numbers”, or “the set of positive Integers”, these are the mathematical objects that set theory allows us to talk about i.e with set theory we can group a collection of objects that have a well defined property. This collection of object is what is called a set and is denoted with \({}\). Basically we can group a collection of objects that have a common property and make a mathematical statement about it which we can then prove to be true or false. Another example is the statement “The sum of first \(n\) odd numbers is \(n^2\)” can you identify the mathematical object here? You’re right if you say odd numbers. Of course odd numbers is a set that contains integers \(x\) that have the property that \(x\) divided by 2 is 1. Integers are mathematical objects too and odd numbers are subsets of Integers.</p> <p>One thing to clarify here is the difference between “a subset of” and “an element of”. Prof. Brendan uses the analogy of a bag here, assuming you have two bags A and B if you take out all the elements of B and you can find them in A, then B “is a subset of” A, notice that B is not a bag inside of A. If it was inside of A then it would be “an element of” A.</p> <p>Set operations that you are familiar with like “Intersection” and “Union” allows us to operate on multiple sets to produce a new one.</p> <p>There are times we need to prove that a set A is equal to a set B and the way we prove that is to show that A is a subset of B and B is a subset of A. This is known as double containment and you would see it used in other chapters. And how do we prove that a set A is a subset of a set B, we simply show that any arbitrary element of A is also an element of B.</p> <p>One very handy thing you’d learn in this chapter is writing in the set builder notation which helps to write clear and concise mathematical proofs</p> <p>Prof. Brendan mentions that interested individuals can dive deeper into set theory and I’ve heard the book <a href="https://www.amazon.co.uk/Naive-Theory-Dover-Books-Mathematics/dp/0486814874/ref=sr_1_1?crid=KRPOHZYERMI0&amp;dib=eyJ2IjoiMSJ9.3EgWTp0-O6PgGV1b6ZjpjtfHTNB7tYIV5lvj1zA9oNDkPZtmm_qrZLnfPQ6KcDyZeKHrEMiY0nQocJsnEDPUBwVnKFdw3Jh7VHc8dVRMPOGElLaCwuJBvO3-2p_roBGsJAmMzIJH1JyvEBSlztBsSFoXQJc4wj4uDP8cVVpmKmGHATNDtguzXAqB9qChgJIz0EozkK1tM4d2V8DhJlZ1oFrO2c4bG2t8GsGXF_wnhmk.8Tk8EYIyumSC5kQTFpO7PVWYtvYfJaH5ZA-N-hhIZp4&amp;dib_tag=se&amp;keywords=naive+set+theory&amp;qid=1710081227&amp;sprefix=naive+set+t%2Caps%2C91&amp;sr=8-1">naive set theory</a> is a good place to start.</p> <h3 id="chapter-4-logic">Chapter 4: Logic</h3> <p>The previous chapter looked at one component of mathematical statements; mathematical objects, this chapter looks at another component which is logic. Both components helps us to write mathematical statements and proofs in a clear and precise manner.</p> <p>Basically logic gives us 2 things;</p> <ol> <li>Notations to express logical ideas in mathematical statements</li> <li>Some rules to make logical inferences about mathematical statements. We already have an intuition on how to do this as humans as shown in our conversations with other humans.</li> </ol> <p>Both of these things alongside the mathematical objects introduced in the previous chapter helps us to communicate our ideas in a clear and precise manner.</p> <p>But what exactly do we mean by mathematical statements/proposition, we’ve seen a few examples. A statement that is grammatically correct AND has just one truth value i.e it can either be true or false, not both or neither, not relatively or subjectively true (maybe this is what differentiates maths from philosophy?). The idea that a mathematical statement can either be true or False is actually one of the axioms of mathematics called “The Law of excluded middle”</p> <p>What about statements like \(x^2 - 1 = 0\) you might ask. These are variable propositions cause their truth values depends on the value of the variable \(x\), e.g this statement is True when the value of \(x\) is 1 but False when the value of \(x\) is 8. Variable propositions like this can be treated as a mathematical statement when we add some constraints to the values that the variable can take.</p> <p>Now that we are clear on what a mathematical statement is let’s now see the role that logic plays in writing and proving them.</p> <p>The first tool logic gives us are Quantifiers.</p> <p>Quantifiers helps us to not just shorten mathematical statements but to also analyse them (as we would soon see). There are two quantifiers, a universal quantifier which connotes phrases like “for all”, “for every” and existential quantifier which connotes statements like “there exists”</p> <p>Here are a few examples of how they are used in mathematical statements.</p> <p>In terms of analysing mathematical statements, “quantification fixes a variable” like when we say for all n, there exists a and b. It’s like having a nested loop where the outer loop fixes a value of n and the inner loop iterates through various values of a and b.</p> <p>Also when we want to contradict a statement in a proof by contradiction we take a logical negation of the statement and assume that to be true. When we use this quantifiers we can easily negate them by turning every “for all” to “there exists”.</p> <p>The next tool that logic gives us are th logical connectives “and”, “or”, “implies”. They help us make more complicated mathematical statements composed from simpler ones.</p> <p>We are familiar with the fact if we have mathematical statements P, and Q</p> <ol> <li>The mathematical statement P and Q holds true if P is True and if Q is true</li> <li>The mathematical statement P or Q holds true when at least one of P and Q is True</li> </ol> <p>A more confusing connective is P ⇒ Q i.e “If P then Q” or “P implies Q” again Dr Brendan’s analogy was super helpful in understanding this.</p> <blockquote> <p>“Let’s say that I make the claim If you work hard, then you will get an A in this course”. Here, P is “You work hard” and Q is “You will get an A”. When can you call me a liar? When can you declare I told the truth? Certainly, if you work hard and get an A, I told the truth. Also, if you work hard and don’t get an A, then I lied to you. However, if you don’t work hard, then no matter what happens, you don’t get to call me a liar! My claim didn’t cover your situation; I was assuming all along you would just work hard! Thus, I didn’t speak an untruth and so, by the Law of the Excluded Middle, I did speak the truth.”</p> </blockquote> <p>Some confusions to clear here</p> <p>The truth value of the statement P ⇒ Q tells us nothing about the truth value of P or the truth value of Q</p> <p>P ⇒ Q does not necessarily connote deduction of the conclusion from the hypothesis. In fact it is possible that P and Q are not connected in any way. Although you would often come across deductive expressions of this in mathematical proofs.</p> <p>P ⇒ Q does not necessarily mean that the converse Q ⇒ P holds this would mean logical equivalence (which I mentioned briefly in the previous chapter).</p> <p>If P and Q are logically equivalent i.e P &lt; = &gt; Q then P and Q have the same truth values regardless of what the truth values are. This is more commonly known as “P if and only P”</p> <p>This is actually more of a biconditional statement because we are talking about two conditional statements at once. Logical equivalence and biconditionals both convey the same idea, the only difference is with logical equivalence you have no idea what P and Q is only that they have the same logical truth value.</p> <p>When we have a biconditional statement “P if and only if Q” what we are really saying is “P if Q” i.e if Q holds then P holds and “P only if Q” i.e P can only hold if Q holds, or If P holds then Q definitely holds. The first is a sufficient condition (Knowing the property Q of a mathematical object is a sufficient condition to conclude the property P or we can guarantee P if we can guarantee Q) while the latter is a necessary condition (For P to even have a chance of holding then Q has to have held, The property Q is essential for P to hold).</p> <p>The rest of this chapter shows some correlations between set operations and logic connectives, then how to prove mathematical statements based on the presence of these logical notations and introduces other proof techniques like proof by contradcition.</p> <h3 id="chapter-6--7-relations-and-functions">Chapter 6 &amp; 7: Relations and Functions</h3> <p>Chapter 6 Introduces relations which help us talk about Functions in chapter 7 in a more mathematical way.</p> <p>Relation is a set that relates the elements of two sets. Relations is a way of encoding information about the elements of two sets. It is a way of comparing elements of 2 sets and saying that they satisfy some property. Think of the function \(x^2\) and its set \({(2,4), (3, 9)}\) this is a way of comparing the set of Natural numbers and saying one number is a square of the other.</p> <p>A function is a special kind of relation</p> <p>Now irrespective of the function or relation defined on two sets, there are certain common properties that they tell us about their elements. These properties are <em>Reflexive, symmetric, transitive, antisymmetric.</em> Whenever we are given a relation on a set we can check if this relation has all of these properties, we can even take a set and try to construct a relation that satisfies these properties.</p> <p>An important type of relation that was extensively discussed in this chapter is the equivalence relation. An equivalence relation is a relation that has the following properties it is reflexive, transitive and symmetric. An equivalence relation partitions a set into equivalence classes. </p> <p>A partition is a way of breaking a large set into smaller sets in a way that they do not overlap. You can think of ann equivalence class as grouping related elements into a club and we can refer to a representative of the club instead of the entire club.</p> <p>A common and practical equivalence relation that you are familiar with but not aware of is that of congruence. The congruence relation says that two integers a and b are related if they have the same remainder when divided by n this is generally written as a=b mod n since this relation is an equivalence relation this means that a and b are in the same equivalent class. Remember that we can refer to an equivalence class by it’s representative instead of the entire class. modn partitions the set of integers Z into equivalence classes.</p> <p>Let a and b are in the same equivalence class when Z is partitioned by modn. Since we can refer to any member of an equivalence class, in the sense of modn, a is equivalent to b.</p> <p>Notice that an integer would be in the same equivalence class as its remainder r since a%n = r and r%n = r. If an integer is very large we can instead refer to its remainder in arithmetic operations. This is why we do modn when we work with very large numbers in our code.</p> <p>The rest of chapter 6 explores other ideas in Modulo Arithmetic Logic and their usefulness in various ways especially in proving interesting properties of integers.</p> <p>What kind of relation is a function? For a function relation the b in the pair(a, b)is unique.</p> <p>A function is a set, this makes sense because they can be graphed where the pair (a, b) are coordinates. Functions can be represented as matrices and the action of a function on a point as matrix multiplication.</p> <p>A function has the following properties</p> <ol> <li>Surjection: for every output there is at least one i.e 1 or more input</li> <li>Injection: for every output there is at most one i.e 0 or more input</li> <li>Bijection: for every output there is exactly one input</li> </ol> <p>When there is a bijection we can talk of reversing the impact of a function i.e applying the inverse of that function</p> <p>Bijections helps us compare the sizes of sets, two sets are the same size if there’s is a bijection relation between them and this idea is used to define finite, infinite, countably infinite and uncountably infinite set.</p> <h3 id="chapter-8-combinatorics">Chapter 8: Combinatorics</h3> <p>Combinatorics is a way of counting the members of a finite set without knowing them. This is the branch of mathematics known as discrete maths. This chapter introduces basic counting principles like rule of sum, rule of products and counting formulas like permutation, combination, binomial coefficients, binomial theorem.</p> <p>Then it looks at more advanced counting techniques such as the pigeonhole principle and Inclusion/exclusion.</p> <h2 id="my-take">My Take</h2> <p>Overall it is a perfect book for someone like me trying to make sense of the subject and Ideal for someone starting a career as a mathematician.</p> <p>The objectives and lookahead were helpful in keeping track of the concepts being learned and how the chapters connect to each other. I also used the objectives as a guide to ensure that I touched the essentials in the chapter summaries of this post.</p> <p>It was long but not unreasonably so, as it contains a lot of examples, and illustrations.</p> <p>There are no solutions to the problems which makes it harder to walk through them as there’s no feedback which is important for someone just trying to learn.</p> <p>My interpretation of notation has greatly improved and I’m more comfortable writing notations myself. When I see a proof now I’m not just trying to understand it, I’m now also trying to map it to a proof technique, why it was conveniently proven this way, are there other ways to prove the same thing? and this has made reading heavily mathematically texts more fun.</p> <h2 id="advice-to-other-readers">Advice to other readers</h2> <p>I recommend walking through all the example problems by yourself (I really enjoyed doing this and I’ve come to fall in love with the sound of my pencil on a white paper when scribbling equations) and at least the exercises after each section. It’s a fast way to help you see what you’ve correctly understood and what you’ve not understood or understood wrongly.</p> <p>Because of the motive I had for reading this book and the tight deadline I had to finish this book I found myself often just concentrating on what I could apply to my work, forgetting to enjoy the pure beauty and wonder that mathematics offers, make it a point to enjoy it that’s the only way to get through the chapters, maybe make a note or creed that you can read every time you pick up this book that reminds you of this.</p> <p>Prof. Brendan has a way of pushing readers to engage more with the text by posing questions in between. This can be both helpful and intimidating especially if this is your first time with mathematical theory. I usually read each chapter 3 times. Initially, I skim for keywords and attempt exercises to spark curiosity. On my second read, I pay closer attention and map out concepts, setting a limit on the effort for understanding challenging parts and noting down unresolved questions. Finally, I focus on grasping difficult concepts and seeking answers to my questions and the ones posed by Prof. Brendan.</p> <p>If you made it all the way here. Thank you for reading!</p>]]></content><author><name></name></author><category term="maths"/><summary type="html"><![CDATA[High level overview of the book World of Abstract Mathematics]]></summary></entry><entry><title type="html">An overview of language model (Part 1)</title><link href="https://tinuademargaret.github.io/blog/2022/language-model-overview-1/" rel="alternate" type="text/html" title="An overview of language model (Part 1)"/><published>2022-11-14T14:14:00+00:00</published><updated>2022-11-14T14:14:00+00:00</updated><id>https://tinuademargaret.github.io/blog/2022/language-model-overview-1</id><content type="html" xml:base="https://tinuademargaret.github.io/blog/2022/language-model-overview-1/"><![CDATA[<p>Language generation by humans is a sequential process. In acts of communication like speaking or writing, language is generated one word at a time and a preceding word or sequence of words would determine the next word depending on the rules of the language in use and the words one is familiar with in that language. A language model mimics this process by predicting the next word given a previous word or sequence of words. It makes this prediction by computing the probability distribution over a vocabulary of words(all the words the model is aware of) which shows how likely each word in the vocabulary would be the next word.</p> <p>For example, given the input sequence “I love playing”, a trained language model is able to predict that the next word is most likely to be “football” rather than “bread”.</p> <p>Language modelling is a fundamental task in NLP and it is used as the basis of other NLP tasks such as machine translation, text summarization etc. Notice that each of these tasks involves the generation of some text given some input context.</p> <p>Prior to the adoption of the use of deep learning techniques in NLP tasks, language modelling was done using statistical modelling with the dominant approach being the n-gram LM.</p> <h2 id="n-gram-language-models">n-gram Language Models</h2> <p>A sequence of n-words is what is said to be an n-gram. In a 1-gram model, the sentence “I am a girl” is chunked into sequences of 1 word as <em>(I) (am) (a) (girl)</em> and each chunk is known as a <em>unigram</em>, likewise in a 2-gram model, the same sentence is chunked as <em>(I am) (am a) (a girl)</em> and each chunk is known as a <em>bigram</em>.</p> <p>An n-gram LM calculates the probability distribution using the Markov assumption that the next word in a sequence only depends on the previous n-1 words. If n is 2 then the probability distribution is estimated on a one-word history. A number of successful n-gram models made use of a value of n=5.</p> <p>n-gram models performed well with small values of n but the performance decreased with larger values of n. This is not far-fetched as the probability of finding long sequences in a text corpus is low and this lowers the chances of the sequence being included as part of the training data. Also with larger values of n, the model becomes increasingly complex. The size of the context being modelled in an n-gram is a limitation to the model as it cannot capture long-range dependencies which are often seen in natural language.</p> <p>The n-gram model also fails to capture semantic relationships between contexts. For example, we can tell that two contexts like “The Tempest was written by William Shakespeare” and “The author of The Tempest is William Shakespeare” mean the same, an n-gram model fails to recognize and capture this similarity.</p> <p>The limitations of the n-gram models have been greatly improved upon by the adoption of deep learning techniques in performing language modelling tasks. These kinds of language models are called Neural Language Models.</p> <h2 id="neural-language-models">Neural Language Models</h2> <p>In this approach, a feature vector usually one dimensional, is associated with each word in a vocabulary, the probability function which is used in n-gram LMs is replaced by a neural network and is now expressed as the conditional probability of predicting the next word given the previous ones. As with neural networks in general, the parameters of the function is tuned in an iterative manner with the objective of minimizing the prediction error of the network.</p> <p>This approach obtained significantly better results compared to the best of the n-gram models. The neural network learns similar words and represents them with similar vector features which makes the language model generalize better. The success of neural language models naturally gives way to more complex neural language models with better performance which we would look at “in my next post”. Irrespective of the architecture of these models, there are some key components that are common to them. These components include:</p> <h3 id="embedding-layer">Embedding Layer</h3> <p>So far we have talked about the input to an LM as a sequence of words and we have talked about calculating the probability of the next word given a sequence of words, but we have not explained how we map words into numerical data to enable the performance of numerical computations.</p> <p>Usually, the mapping of words to numerical data known as <em>Word embeddings</em> is done via <em>one hot encoding</em>. In one hot encoding, every word in a vocabulary is represented as a vector of size |V| which is the length of the vocabulary(i.e the total number of words that needs to be encoded). The position of the particular word being represented is set to a value of 1 while every other word’s position in the vocabulary is set to 0. This data quickly becomes large as the size of the vocabulary increases. Also when this data is fed into a neural network a lot of computation goes on for data that is mostly zeros which is quite inefficient.</p> <p>An improvement to this was the addition of an embedding layer (the feature vector associated with each word described above) to the neural network. The embedding layer in this case is randomly initialised and jointly trained with the rest of the network layers. The matrix formed by the feature vectors then forms an embedding lookup table |E| of length |V|.</p> <p>The <a href="https://arxiv.org/pdf/1301.3781.pdf">word2vec paper</a> proposed a word embedding model, which pre-trains word embeddings that are used to initialise the word embedding layer in an LM. This further improved the performance of neural language models on a range of NLP tasks.</p> <h3 id="softmax-layer">Softmax Layer</h3> <p>Language modelling can be seen as a multiclass classification task with the number of classes equal to the length of the vocabulary |V|. To make the output of the LM a probability distribution over a vocabulary of words, the softmax layer has weights similar to the embedding layer and the softmax activation function is used to transform the output into a probability distribution.</p> <p>In a continuation post we would look at different Neural Model Architectures including the recent and popular Transformers.</p>]]></content><author><name></name></author><category term="AI"/><summary type="html"><![CDATA[Where we started]]></summary></entry><entry><title type="html">An overview of language model (Part 2)</title><link href="https://tinuademargaret.github.io/blog/2022/language-model-overview-2/" rel="alternate" type="text/html" title="An overview of language model (Part 2)"/><published>2022-11-14T14:14:00+00:00</published><updated>2022-11-14T14:14:00+00:00</updated><id>https://tinuademargaret.github.io/blog/2022/language-model-overview-2</id><content type="html" xml:base="https://tinuademargaret.github.io/blog/2022/language-model-overview-2/"><![CDATA[<p>In the previous post we looked at how languge modelling was done with n-gram models and some key components of neural language models namely the embedding layer and the softmax layer.</p> <p>While the Embedding layer and the softmax layer are common to LMs in general, the network layer varies and is a distinguishing factor in the capabilities of LMs. In this post, we’ll look at two major architectures namely Recurrent Neural Networks (RNN) and Transformer Networks.</p> <h2 id="reccurent-neural-networks">Reccurent Neural Networks</h2> <p>The major difference between a vanilla Neural Network commonly referred to as a Feed Forward Neural Network (FFNN) and an RNN, is the incorporation of memory (the output of hidden layer neurons) as an additional input to the network during the next step training on sequential data. The layers in an RNN contain RNN cells as opposed to neurons seen in FFNNs. The output of an RNN cell at time <em>t</em> depends on the current inputs, their weights, and also the output of the hidden layer in the previous time step.</p> <p>Because of the additional memory component, the error of the output of the model is usually backpropagated through previous timesteps and if backpropagation is done for more than approximately ten timesteps the gradient can become either very small leading to a vanishing gradient problem or grow uncontrollably leading to an exploding gradient problem. The vanishing gradient problem especially hinders the capability of the model to capture information several time steps back.</p> <p>Long Short-Term Memory Networks (LSTMs) and Gated Recurrent Networks (GRNs) are variants of RNNs proposed to solve the vanishing gradient problem. Christopher Olah gives a good breakdown on RNNs <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">here</a>.</p> <h2 id="sequence-to-sequence-models">Sequence-to-Sequence models</h2> <p>The encoder-decoder sequence-to-sequence model was originally proposed to overcome the limitations of performing sequence-to-sequence mapping with neural networks especially in machine translation tasks.</p> <p>For an RNN to process sequential data the dimensions of the input and output have to be known and fixed, but in many cases, problems are best expressed as sequences whose lengths are not previously known, or whose input sequence length varies from the output sequence length such as in machine translation.</p> <p>In a vanilla sequence-to-sequence model, the encoder and decoder is usually any of the RNNs mentioned above. The encoder takes an entire input sentence and maps it to a fixed-sized vector. This vector is then fed into the decoder which then maps the vector to a target sequence.</p> <p>Let’s talk a bit about the attention mechanism.</p> <h3 id="attention-mechanism">Attention mechanism</h3> <p>Attention mechanism in machine learning originated from the observation that humans process sensory information in a sequential manner, paying special attention to a part of the whole information at a time. Researchers have used the Attention mechanism to replicate this behaviour in neural networks enabling them to focus on a subset of input information at a time. For example, in image captioning, a computer vision network can focus on different parts of the image at a time to produce a more accurate caption.</p> <p>Attention between two sequences was initially introduced in a neural machine translation task. Before the use of attention, when performing machine translation tasks using sequence-to-sequence models, as we have described above, the entire input sequence is passed as a vector from the encoder to the decoder. Still, with attention, the encoder passes information about each word it processes to the decoder, but the decoder can focus on words as they become important.</p> <p>To induce attention, the decoder scores the set of vectors (each associated with a word in the input sequence) from the encoder at each time step. It applies a softmax function to create an attention distribution. Each vector is then multiplied by its softmax score, drowning out vectors with lower scores and amplifying vectors with higher scores. There is a nice visualization of sequence to sequence models by Jay Alammar <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">here</a></p> <p>The attention mechanism introduced in vanilla sequence-to-sequence models made it compelling, but there was still an issue of computational efficiency since computation was done sequentially using recurrent models. This led to a proposal for the Transformer to reduce the sequential computation in sequential modelling tasks.</p> <h2 id="transformers">Transformers</h2> <p>The original proposed transformer is composed of an encoder-decoder network. It removes which the recurrent connections seen in sequence to sequence models, and uses the attention mechanism to model information across long sequences. The function of the encoder and decoder in a transformer network remains just the same as in a vanilla sequence-to-sequence model but the transformer network uses not just an encoder and a decoder but a stack of encoders and decoders. Each encoder and decoder can be seen as a layer of the transformer network. Each layer of the decoder is made up of two sub-layers, a multi-head attention layer and a feed-forward layer which is essentially a feed-forward neural network. A Layer normalisation is also included around the sub-layers which helps to improve the training speed of the model. In addition to the sub-layers seen in the encoder, the decoder has a third sub-layer for performing multi-head attention over the output of the encoder stack.</p> <p>Instead of relying on the output of the previous state to encode information about the last step time in the current time step, attention is used in transformers to encode information about relevant words into a word currently being processed. The process to achieve this is similar to the one described above between an encoder and a decoder in a vanilla sequence-to-sequence model with attention. In this case, we score each word in an input sentence against a word currently being processed, and this score determines how much focus should be placed on the other words in the input sentence as one is currently encoded. A softmax function is then applied to the scores. A value vector for each word is then multiplied by the softmax score for each word, again drowning out vectors with lower scores and amplifying vectors with higher scores. The vectors are then summed up to produce the output of the attention layer in an encoder and fed into the feed-forward layer. This calculation is done multiple times to create \emph{multi-headed} attention and improves the performance of the attention layer.</p> <h3 id="positional-embeddings">Positional Embeddings</h3> <p>Transformers need a way to represent the position of words in a sequence of input since they do not have recurrent connections. To address this, the proposed model adds a vector to each input word embedding called a position embedding that helps it know the position of each word and the distance from each word. The positional embedding follows a specific pattern and can either be learned or determined by a pre-defined function. The sinusoid function was used in the original implementation.</p> <p>The Transformers, since its inception, has been highly adopted as a model architecture for LMs. Some popular based Transformer based language models include BERT, RETRO, GPT-2 and its successors.</p>]]></content><author><name></name></author><category term="AI"/><summary type="html"><![CDATA[Where we are now]]></summary></entry><entry><title type="html">Not any less optimistic</title><link href="https://tinuademargaret.github.io/blog/2022/optimism/" rel="alternate" type="text/html" title="Not any less optimistic"/><published>2022-11-04T14:14:00+00:00</published><updated>2022-11-04T14:14:00+00:00</updated><id>https://tinuademargaret.github.io/blog/2022/optimism</id><content type="html" xml:base="https://tinuademargaret.github.io/blog/2022/optimism/"><![CDATA[<p>I live life like everything is certain. If you could take a glimpse into my life, you’d see that everything that can be planned has been planned. I plan with so much precision, the nitty gritty details, what to eat, what to wear, even when to drink water.</p> <p>The problem is that life is so uncertain (a big problem in AI btw), and when my plans gets intruded by life I break down. I’m almost always in tears and today is one of those days. Instead of crying today I decided to do some reflection and questioning this mindset of mine that cannot handle uncertainty.</p> <p>I think one attribute of mine that fundammentally affects the way I plan is that I’m dogheaded optimistic. I fundamentally believe that ANYTHING can be achieved so I only focus on the good, and plan for the best case scenarios.</p> <p>What about the worst cases? Why am I blinded by my optimisim to see that even though the best is possible the worse is also possible. Are the worse cases bad? Am I suppossed to plan for them not to happen or plan on what to do when they happen?</p> <p>Well, maybe worse cases are not neccesarily a bad thing. Yeah, they may seem bad in the moment, but in the long run and if handled properly they can eventually lead to something good.</p> <p>I don’t think life is conscious of what is good and bad, life is a series of events and how we interprete an event is what makes it good or bad.</p> <p>So planning how to navigate and manage worse case scenarios shouldn’t make you less optimistic, instead it would make you a better planner and would keep the bad days away.</p> <p>Coincidentally, I opened one of my favorite newsletters, 3-2-1 by James Clear this morning and guess the first quote?</p> <p>“The ultimate form of preparation is not planning for a specific scenario, but a mindset that can handle uncertainty”.</p> <p>The worse case scenario is not the enemy, not planning for them is. Embrace them! Don’t be afraid to ask the question “What if X goes wrong?” What’s fun about everything always going according to plan anyways :)</p>]]></content><author><name></name></author><category term="life"/><summary type="html"><![CDATA[Handling uncertainties]]></summary></entry></feed>